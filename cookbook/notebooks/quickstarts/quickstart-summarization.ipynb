{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using GenAI Foundational Platform Endpoints for Summarization (Using Claude 2)***\n",
    "\n",
    "Following is sample that summarizes each individual page of a document and creates a summary of summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, make sure you create a .env file in the same folder as the notebook, and have the following variables. You can get the values for these variables from your admin of the platform.\n",
    "\n",
    " \n",
    " COGNITO_CLIENT_ID='<replace_me>'\n",
    "\n",
    " COGNITO_CLIENT_SECRET='<replace_me>'\n",
    "\n",
    " COGNITO_USER_POOL_ID='<replace_me>'\n",
    "\n",
    " COGNITO_REGION='<replace_me>'\n",
    "\n",
    " COGNITO_DOMAIN='<replace_me>'\n",
    " \n",
    " PLATFORM_API_URL='<replace_me>'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***Note .env file is only needed when running a notebook. In a real application deployed to EC2 or container, you can just create environment variables. (For example using export command)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r reqs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import pprint\n",
    "# Load the environment variables. This is only necessary if you are using a .env file to store your credentials.\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inititalize values from env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "APP_CLIENT_ID = os.getenv('COGNITO_CLIENT_ID')\n",
    "APP_USER_POOL_ID = os.getenv('COGNITO_USER_POOL_ID')\n",
    "APP_CLIENT_SECRET = os.getenv('COGNITO_CLIENT_SECRET')\n",
    "REGION = os.getenv('COGNITO_REGION')\n",
    "DOMAIN = os.getenv('COGNITO_DOMAIN')\n",
    "BASE_URL = os.getenv('PLATFORM_API_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create reusable get and post methods to make API calls to the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def get(proxy=None, token=None):\n",
    "    url = BASE_URL\n",
    "    if proxy:\n",
    "        url = BASE_URL + '/' + proxy\n",
    "\n",
    "    if token:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "    response = requests.get(url, headers=headers, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "def post(data, proxy=None, token=None):\n",
    "    url = BASE_URL\n",
    "    if proxy:\n",
    "        url = BASE_URL + '/' + proxy\n",
    "    \n",
    "    if token:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data), timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with cognito and get the access token. We use this token in the header to make calls to the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CognitoTokenManager, get_cognito_public_keys\n",
    "import pprint\n",
    "cognito_token_manager = CognitoTokenManager(APP_CLIENT_ID, APP_CLIENT_SECRET, APP_USER_POOL_ID, REGION, DOMAIN)\n",
    "token = cognito_token_manager._fetch_token_with_secret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Extraction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_extraction_job_endpoint = 'document/extraction/create_job'\n",
    "extraction_job = get(proxy=create_extraction_job_endpoint, token=token)\n",
    "pprint.pprint(extraction_job.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Files to the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_file_endpoint = 'document/extraction/register_file'\n",
    "file_name = '<REPLACE_WITH_YOUR_FILE_PATH>' # eg. 'data/your_file.pdf'\n",
    "data = { \n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id'], \n",
    "    \"file_name\": file_name\n",
    "}\n",
    "response = post(proxy=register_file_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files using presigned urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the file to the S3 bucket\n",
    "pre_signed_url = response.json()['upload_url']\n",
    "import requests\n",
    "with open(file_name, 'rb') as f:\n",
    "    response = requests.put(pre_signed_url, data=f)\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Extraction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start job\n",
    "start_job_endpoint = 'document/extraction/start_job'\n",
    "data = {\n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id']\n",
    "}\n",
    "response = post(proxy=start_job_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Extraction Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /document/extraction/job_status/{extraction_job_id}\n",
    "import time\n",
    "job_status_endpoint = f'document/extraction/job_status/{extraction_job.json()[\"extraction_job_id\"]}'\n",
    "response = get(proxy=job_status_endpoint, token=token)\n",
    "status = response.json()['status']\n",
    "while status != 'COMPLETED' and status != 'FAILED' and status != 'COMPLETED_WITH_ERRORS':\n",
    "    response = get(proxy=job_status_endpoint, token=token)\n",
    "    status = response.json()['status']\n",
    "    print(status)\n",
    "    time.sleep(5)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /document/extraction/file_status\n",
    "file_status_endpoint = 'document/extraction/file_status'\n",
    "data = {\n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id'],\n",
    "    \"file_name\": file_name\n",
    "}\n",
    "response = post(proxy=file_status_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())\n",
    "result_url = response.json()['result_url']\n",
    "\n",
    "# Get the result\n",
    "response = requests.get(result_url)\n",
    "print(response.status_code)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chunking job. Chunking by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /document/chunking/create_job\n",
    "create_chunking_job_endpoint = 'document/chunking/create_job'\n",
    "chunking_strategy = 'page'\n",
    "chunk_size = 400\n",
    "chunk_overlap = 100\n",
    "data = {\n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id'],\n",
    "    \"chunking_strategy\": chunking_strategy\n",
    "}\n",
    "chunk_job = post(proxy=create_chunking_job_endpoint, token=token, data=data)\n",
    "pprint.pprint(chunk_job.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Chunking Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /document/chunking/job_status/{job_id}\n",
    "import time\n",
    "job_status_endpoint = f'document/chunking/job_status/{chunk_job.json()[\"chunking_job_id\"]}'\n",
    "chunk_job_status = get(proxy=job_status_endpoint, token=token)\n",
    "status = chunk_job_status.json()['status']\n",
    "while status != 'COMPLETED' and status != 'FAILED' and status != 'COMPLETED_WITH_ERRORS':\n",
    "    chunk_job_status = get(proxy=job_status_endpoint, token=token)\n",
    "    status = chunk_job_status.json()['status']\n",
    "    print(status)\n",
    "    time.sleep(5)\n",
    "pprint.pprint(chunk_job_status.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /document/chunking/chunk_file_url\n",
    "chunk_file_url_endpoint = 'document/chunking/chunk_file_url'\n",
    "data = {\n",
    "    \"chunking_job_id\": chunk_job.json()['chunking_job_id'],\n",
    "    \"file_name\": file_name\n",
    "}\n",
    "chunk_file = post(proxy=chunk_file_url_endpoint, token=token, data=data)\n",
    "pprint.pprint(chunk_file.text)\n",
    "chunk_file_url = chunk_file.json()['chunk_file_url']\n",
    "\n",
    "# Get the chunked file\n",
    "chunk_file_text = requests.get(chunk_file_url)\n",
    "print(chunk_file_text.status_code)\n",
    "pprint.pprint(chunk_file_text.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print All Extracted Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for chunk in chunk_file_text.json():\n",
    "    pages.append(chunk['chunk'])\n",
    "\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create individual summary for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"Summarize the following text in one or two paragraphs: {text}\"\n",
    "summaries = []\n",
    "invoke_model_endpoint = 'model/invoke'\n",
    "page_num = 1\n",
    "for page in pages:\n",
    "    prompt = summary_prompt.format(text=page)\n",
    "    data = { \n",
    "        \"model_name\": \"ANTHROPIC_CLAUDE_V2\", \n",
    "        \"prompt\": prompt, \n",
    "        \"max_tokens\": 1000, \n",
    "        \"temperature\": 0.7, \n",
    "        \"stop_sequences\": [\"\\\\n\"] \n",
    "    }\n",
    "    response = post(proxy=invoke_model_endpoint, token=token, data=data)\n",
    "    print(f\"Page {page_num}\")\n",
    "    page_num += 1\n",
    "    print(response.json()['output_text'])\n",
    "    summaries.append(response.json()['output_text'])\n",
    "\n",
    "pprint.pprint(summaries)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary = \" \".join(summaries)\n",
    "prompt = \"Following is a list of summaries of each page of a document. Combine the summaries in to one summary of atleast 4 paragraphs. Don't miss details: {text}\".format(text=all_summary)+ \"Summary:\"\n",
    "data = { \n",
    "        \"model_name\": \"ANTHROPIC_CLAUDE_V2\", \n",
    "        \"prompt\": prompt, \n",
    "        \"max_tokens\": 1000, \n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "response = post(proxy=invoke_model_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json()['output_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
