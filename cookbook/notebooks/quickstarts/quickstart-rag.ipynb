{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using GenAI Foundational Platform Endpoints for RAG***\n",
    "\n",
    "Following is sample that shows how to build a RAG workflow using GenAI Foundational Architecture Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, make sure you create a .env file in the same folder as the notebook, and have the following variables. You can get the values for these variables from your admin of the platform.\n",
    "\n",
    " \n",
    " COGNITO_CLIENT_ID='<replace_me>'\n",
    "\n",
    " COGNITO_CLIENT_SECRET='<replace_me>'\n",
    "\n",
    " COGNITO_USER_POOL_ID='<replace_me>'\n",
    "\n",
    " COGNITO_REGION='<replace_me>'\n",
    "\n",
    " COGNITO_DOMAIN='<replace_me>'\n",
    " \n",
    " PLATFORM_API_URL='<replace_me>'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***Note .env file is only needed when running a notebook. In a real application deployed to EC2 or container, you can just create environment variables. (For example using export command)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r reqs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import pprint\n",
    "import time\n",
    "# Load the environment variables. This is only necessary if you are using a .env file to store your credentials.\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inititalize values from env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "APP_CLIENT_ID = os.getenv('COGNITO_CLIENT_ID')\n",
    "APP_USER_POOL_ID = os.getenv('COGNITO_USER_POOL_ID')\n",
    "APP_CLIENT_SECRET = os.getenv('COGNITO_CLIENT_SECRET')\n",
    "REGION = os.getenv('COGNITO_REGION')\n",
    "DOMAIN = os.getenv('COGNITO_DOMAIN')\n",
    "BASE_URL = os.getenv('PLATFORM_API_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create reusable get and post methods to make API calls to the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def get(proxy=None, token=None):\n",
    "    url = BASE_URL\n",
    "    # if url ends with /, remove it\n",
    "    if url.endswith('/'):\n",
    "        url = url[:-1]\n",
    "    if proxy:\n",
    "        url = url + '/' + proxy\n",
    "\n",
    "    if token:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "    response = requests.get(url, headers=headers, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "def post(data, proxy=None, token=None):\n",
    "    url = BASE_URL\n",
    "    if url.endswith('/'):\n",
    "        url = url[:-1]\n",
    "    if proxy:\n",
    "        url = url + '/' + proxy\n",
    "    \n",
    "    if token:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data), timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with cognito and get the access token. We use this token in the header to make calls to the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CognitoTokenManager, get_cognito_public_keys\n",
    "import pprint\n",
    "cognito_token_manager = CognitoTokenManager(APP_CLIENT_ID, APP_CLIENT_SECRET, APP_USER_POOL_ID, REGION, DOMAIN)\n",
    "token = cognito_token_manager._fetch_token_with_secret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /model/list_models\n",
    "list_model_endpoint = 'model/list_models'\n",
    "response = get(proxy=list_model_endpoint, token=token)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /model/invoke\n",
    "invoke_model_endpoint = 'model/invoke'\n",
    "data = { \n",
    "    \"model_name\": \"ANTHROPIC_CLAUDE_V2\", \n",
    "    \"prompt\": \"Translate the following text to French: 'Hello, how are you?'\", \n",
    "    \"max_tokens\": 100, \n",
    "    \"temperature\": 0.7, \n",
    "    \"top_p\": 0.9, \n",
    "    \"top_k\": 50, \n",
    "    \"stop_sequences\": [\"\\\\n\"] \n",
    "}\n",
    "response = post(proxy=invoke_model_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /model/invoke\n",
    "data = { \n",
    "    \"model_name\": \"ANTHROPIC_CLAUDE_V2\", \n",
    "    \"prompt\": [ \n",
    "        { \n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"text\": \"What is the weather like today?\"}] \n",
    "        }, \n",
    "        { \n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": [{\"text\": \"The weather is sunny with a high of 25Â°C.\"}] \n",
    "        } \n",
    "    ], \n",
    "    \"max_tokens\": 100, \n",
    "    \"temperature\": 0.7, \n",
    "    \"top_p\": 0.9, \n",
    "    \"top_k\": 50, \n",
    "    \"stop_sequences\": [\"\\\\n\"], \n",
    "    \"system_prompts\": [ \n",
    "        { \n",
    "            \"text\": \"You are a helful assistant.\" \n",
    "        } \n",
    "    ] \n",
    "}\n",
    "response = post(proxy=invoke_model_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /model/embed\n",
    "embed_text_endpoint = 'model/embed'\n",
    "data = { \n",
    "    \"model_name\": \"TITAN_TEXT_EMBED_V2\", \n",
    "    \"input_text\": \"Hello, how are you?\" \n",
    "}\n",
    "response = post(proxy=embed_text_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Extraction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_extraction_job_endpoint = 'document/extraction/create_job'\n",
    "extraction_job = get(proxy=create_extraction_job_endpoint, token=token)\n",
    "pprint.pprint(extraction_job.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Files to the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_file_endpoint = 'document/extraction/register_file'\n",
    "file_name = '<REPLACE_WITH_LOCAL_FILE_PATH>' # e.g. 'data/sample.pdf'\n",
    "data = { \n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id'], \n",
    "    \"file_name\": file_name\n",
    "}\n",
    "response = post(proxy=register_file_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files using presigned urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the file to the S3 bucket\n",
    "pre_signed_url = response.json()['upload_url']\n",
    "import requests\n",
    "with open(file_name, 'rb') as f:\n",
    "    response = requests.put(pre_signed_url, data=f)\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Extraction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start job\n",
    "start_job_endpoint = 'document/extraction/start_job'\n",
    "data = {\n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id']\n",
    "}\n",
    "response = post(proxy=start_job_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Extraction Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /document/extraction/job_status/{extraction_job_id}\n",
    "job_status_endpoint = f'document/extraction/job_status/{extraction_job.json()[\"extraction_job_id\"]}'\n",
    "response = get(proxy=job_status_endpoint, token=token)\n",
    "status = response.json()['status']\n",
    "while status != 'COMPLETED' and status != 'FAILED' and status != 'COMPLETED_WITH_ERRORS':\n",
    "    response = get(proxy=job_status_endpoint, token=token)\n",
    "    status = response.json()['status']\n",
    "    print(status)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /document/extraction/file_status\n",
    "file_status_endpoint = 'document/extraction/file_status'\n",
    "data = {\n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id'],\n",
    "    \"file_name\": file_name\n",
    "}\n",
    "response = post(proxy=file_status_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())\n",
    "result_url = response.json()['result_url']\n",
    "\n",
    "# Get the result\n",
    "response = requests.get(result_url)\n",
    "print(response.status_code)\n",
    "pprint.pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chunking job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /document/chunking/create_job\n",
    "create_chunking_job_endpoint = 'document/chunking/create_job'\n",
    "chunking_strategy = 'fixed_size'\n",
    "chunk_size = 400\n",
    "chunk_overlap = 100\n",
    "data = {\n",
    "    \"extraction_job_id\": extraction_job.json()['extraction_job_id'],\n",
    "    \"chunking_strategy\": chunking_strategy,\n",
    "    \"chunking_params\": {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap\n",
    "    }\n",
    "}\n",
    "chunk_job = post(proxy=create_chunking_job_endpoint, token=token, data=data)\n",
    "pprint.pprint(chunk_job.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Chunking Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /document/chunking/job_status/{job_id}\n",
    "job_status_endpoint = f'document/chunking/job_status/{chunk_job.json()[\"chunking_job_id\"]}'\n",
    "chunk_job_status = get(proxy=job_status_endpoint, token=token)\n",
    "status = chunk_job_status.json()['status']\n",
    "while status != 'COMPLETED' and status != 'FAILED' and status != 'COMPLETED_WITH_ERRORS':\n",
    "    chunk_job_status = get(proxy=job_status_endpoint, token=token)\n",
    "    status = chunk_job_status.json()['status']\n",
    "    print(status)\n",
    "pprint.pprint(chunk_job_status.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /document/chunking/chunk_file_url\n",
    "chunk_file_url_endpoint = 'document/chunking/chunk_file_url'\n",
    "data = {\n",
    "    \"chunking_job_id\": chunk_job.json()['chunking_job_id'],\n",
    "    \"file_name\": file_name\n",
    "}\n",
    "chunk_file = post(proxy=chunk_file_url_endpoint, token=token, data=data)\n",
    "pprint.pprint(chunk_file.text)\n",
    "chunk_file_url = chunk_file.json()['chunk_file_url']\n",
    "\n",
    "# Get the chunked file\n",
    "chunk_file_text = requests.get(chunk_file_url)\n",
    "print(chunk_file_text.status_code)\n",
    "pprint.pprint(chunk_file_text.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /vector/store/create\n",
    "create_vector_store_endpoint = 'vector/store/create'\n",
    "data = {\n",
    "  \"store_name\": \"SolarSystem\",\n",
    "  \"store_type\": \"opensearchserverless\",\n",
    "  \"description\": \"Collection for storing vectorized documents\",\n",
    "  \"tags\": [\n",
    "    {\n",
    "      \"key\": \"project\",\n",
    "      \"value\": \"GenerativeAI\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "vector_store = post(proxy=create_vector_store_endpoint, token=token, data=data)\n",
    "pprint.pprint(vector_store.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Vector Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /vector/store/status\n",
    "vector_store_status_endpoint = 'vector/store/status'\n",
    "data = {\n",
    "    \"store_id\": vector_store.json()['store_id']\n",
    "}\n",
    "vector_store_status = post(proxy=vector_store_status_endpoint, token=token, data=data)\n",
    "pprint.pprint(vector_store_status.json())\n",
    "while vector_store_status.json()['status'] != 'ACTIVE':\n",
    "    vector_store_status = post(proxy=vector_store_status_endpoint, token=token, data=data)\n",
    "    pprint.pprint(vector_store_status.json())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /vector/store/index/create POST\n",
    "create_index_endpoint = 'vector/store/index/create'\n",
    "data = {\n",
    "  \"store_id\": vector_store.json()['store_id'],\n",
    "  \"index_name\": \"my_index2\"\n",
    "}\n",
    "vector_index = post(proxy=create_index_endpoint, token=token, data=data)\n",
    "pprint.pprint(vector_index.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Index Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /vector/store/index/create\n",
    "create_index_endpoint = 'vector/store/index/create'\n",
    "data = {\n",
    "  \"store_id\": vector_store.json()['store_id'],\n",
    "  \"index_name\": \"my_index3\"\n",
    "}\n",
    "vector_index = post(proxy=create_index_endpoint, token=token, data=data)\n",
    "pprint.pprint(vector_index.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /vector/store/vectorize\n",
    "vectorize_endpoint = 'vector/store/vectorize'\n",
    "data = {\n",
    "  \"chunking_job_id\": chunk_job.json()['chunking_job_id'],\n",
    "  \"index_id\": vector_index.json()['index_id']\n",
    "}\n",
    "vectorize = post(proxy=vectorize_endpoint, token=token, data=data)\n",
    "pprint.pprint(vectorize.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /vector/job/status/{vectorize_job_id}\n",
    "job_status_endpoint = f'vector/job/status/{vectorize.json()[\"vectorize_job_id\"]}'\n",
    "vectorize_job_status = get(proxy=job_status_endpoint, token=token)\n",
    "status = vectorize_job_status.json()['status']\n",
    "while status != 'COMPLETED' and status != 'FAILED' and status != 'COMPLETED_WITH_ERRORS':\n",
    "    vectorize_job_status = get(proxy=job_status_endpoint, token=token)\n",
    "    status = vectorize_job_status.json()['status']\n",
    "    print(status)\n",
    "pprint.pprint(vectorize_job_status.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST /vector/search\n",
    "search_endpoint = 'vector/search'\n",
    "data = {\n",
    "  \"query\": \"<REPLACE_WITH_QUERY>\", # Question related to the document\n",
    "  \"index_id\": vector_index.json()['index_id']\n",
    "}\n",
    "search = post(proxy=search_endpoint, token=token, data=data)\n",
    "pprint.pprint(search.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"<REPLACE_WITH_QUERY>\" # Question related to the document\n",
    "\n",
    "# Vector search\n",
    "search_endpoint = 'vector/search'\n",
    "data = {\n",
    "  \"query\": question,\n",
    "  \"index_id\": vector_index.json()['index_id']\n",
    "}\n",
    "search = post(proxy=search_endpoint, token=token, data=data)\n",
    "\n",
    "prompt = \"\"\"\n",
    "           You are a helpful assistant. Given a context, answer the following question.\n",
    "           Context: {context}\n",
    "           Question: {question}\n",
    "           Answer:\n",
    "           \"\"\"\n",
    "context_text = \"\"\n",
    "for hit in search.json():\n",
    "    context_text += hit['text'] + ' '\n",
    "    context_text += \" \"\n",
    "final_prompt = prompt.format(context=context_text, question=question)\n",
    "print(final_prompt)\n",
    "\n",
    "# Invoke the model\n",
    "data = { \n",
    "    \"model_name\": \"ANTHROPIC_CLAUDE_V2\", \n",
    "    \"prompt\": final_prompt, \n",
    "    \"max_tokens\": 100, \n",
    "    \"temperature\": 0.7, \n",
    "    \"top_p\": 0.9, \n",
    "    \"top_k\": 50, \n",
    "    \"stop_sequences\": [\"\\\\n\"] \n",
    "}\n",
    "response = post(proxy=invoke_model_endpoint, token=token, data=data)\n",
    "pprint.pprint(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
