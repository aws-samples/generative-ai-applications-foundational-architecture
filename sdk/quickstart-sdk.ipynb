{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GenAI Foundational Platform Endpoints for RAG (Using the SDK (accelerator.py))\n",
    "\n",
    "Following is sample that shows how to build a RAG workflow using GenAI Foundational Platform Endpoints. This uses the SDK file (accelerator.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, make sure you create a .env file in the same folder as the notebook, and have the following variables:\n",
    "\n",
    " \n",
    " COGNITO_CLIENT_ID='<replace_me>'\n",
    "\n",
    " COGNITO_CLIENT_SECRET='<replace_me>'\n",
    "\n",
    " COGNITO_USER_POOL_ID='<replace_me>'\n",
    "\n",
    " COGNITO_REGION='<replace_me>'\n",
    "\n",
    " COGNITO_DOMAIN='<replace_me>'\n",
    " \n",
    " PLATFORM_API_URL='<replace_me>'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***Note .env file is only needed when running a notebook. In a real application deployed to EC2 or container, you can just create environment variables. (For example using export command)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r reqs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the SDK from accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerator import GenerativeAIAccelerator\n",
    "import os\n",
    "import dotenv\n",
    "import pprint\n",
    "# Load the environment variables. This is only necessary if you are using a .env file to store your credentials.\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Accelerator and services sdk\n",
    "accelerator = GenerativeAIAccelerator()\n",
    "_health = accelerator.health_service\n",
    "_model = accelerator.model_service\n",
    "_document = accelerator.document_service\n",
    "_vectors = accelerator.vector_service\n",
    "_prompt = accelerator.prompt_service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the acceleraor instance and checking the service status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "services = ['model', 'document', 'prompt', 'vector']\n",
    "for service in services:\n",
    "    print(service+\":\"+_health.check_health(service)['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models = _model.list_models()\n",
    "print(list_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name=\"ANTHROPIC_CLAUDE_V2\", \n",
    "prompt=\"Translate the following text to French: 'Hello, how are you?'\", \n",
    "max_tokens=100, \n",
    "temperature=0.7, \n",
    "top_p=0.9, \n",
    "top_k=50, \n",
    "stop_sequences=[\"\\\\n\"] \n",
    "response = _model.invoke_model(model_name=\"ANTHROPIC_CLAUDE_V2\", prompt=\"Translate the following text to French: 'Hello, how are you?'\", max_tokens=100, temperature=0.7, top_p=0.9, top_k=50, stop_sequences=[\"\\\\n\"])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = [ \n",
    "        { \n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"text\": \"What is the weather like today?\"}] \n",
    "        }, \n",
    "        { \n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": [{\"text\": \"The weather is sunny with a high of 25Â°C.\"}] \n",
    "        } \n",
    "    ]\n",
    "\n",
    "system_prompts = [ \n",
    "        { \n",
    "            \"text\": \"You are a helful assistant.\" \n",
    "        } \n",
    "    ] \n",
    "\n",
    "response = _model.invoke_model(model_name=\"ANTHROPIC_CLAUDE_V2\", prompt=prompt, max_tokens=100, temperature=0.7, top_p=0.9, top_k=50, stop_sequences=[\"\\\\n\"], system_prompts=system_prompts)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TITAN_TEXT_EMBED_V2\", \n",
    "input_text = \"Hello, how are you?\" \n",
    "response = _model.invoke_embed(model_name=\"TITAN_TEXT_EMBED_V2\", input_text=\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Extraction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_job = _document.create_extraction_job()\n",
    "pprint.pprint(extraction_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Files to the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '<REPLACE_WITH_YOUR_FILE_PATH>' # eg. 'data/sample.pdf'\n",
    "response = _document.register_file_for_extraction(extraction_job_id=extraction_job['extraction_job_id'], file_name=file_name)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files using presigned urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the file to the S3 bucket\n",
    "pre_signed_url = response['upload_url']\n",
    "import requests\n",
    "with open(file_name, 'rb') as f:\n",
    "    response = requests.put(pre_signed_url, data=f)\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Extraction Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _document.start_extraction_job(extraction_job_id=extraction_job['extraction_job_id'])\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Extraction Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "response = _document.get_extraction_job_status(extraction_job_id=extraction_job['extraction_job_id'])\n",
    "job_status = response['status']\n",
    "print(job_status)\n",
    "while job_status != 'COMPLETED' and job_status != 'FAILED' and job_status != 'COMPLETED_WITH_ERRORS':\n",
    "    response = _document.get_extraction_job_status(extraction_job_id=extraction_job['extraction_job_id'])\n",
    "    job_status = response['status']\n",
    "    print(job_status)\n",
    "    time.sleep(5)\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _document.get_file_status(extraction_job_id=extraction_job['extraction_job_id'], file_name=file_name)\n",
    "text = requests.get(response['result_url']).json()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chunking job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_params = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 100\n",
    "}\n",
    "chunk_job = _document.create_chunking_job(extraction_job_id=extraction_job['extraction_job_id'], chunking_strategy='fixed_size', chunking_params=chunking_params)\n",
    "pprint.pprint(chunk_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Chunking Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _document.get_chunking_job_status(job_id=chunk_job['chunking_job_id'])\n",
    "pprint.pprint(response)\n",
    "while response['status'] != 'COMPLETED' and response['status'] != 'FAILED' and response['status'] != 'COMPLETED_WITH_ERRORS':\n",
    "    response = _document.get_chunking_job_status(job_id=chunk_job['chunking_job_id'])\n",
    "    pprint.pprint(response)\n",
    "    time.sleep(5)\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _document.get_chunking_results(chunking_job_id=chunk_job['chunking_job_id'], file_name=file_name)\n",
    "pprint.pprint(response)\n",
    "\n",
    "# Get the chunked file\n",
    "chunk_file_text = requests.get(response['chunk_file_url'])\n",
    "print(chunk_file_text.status_code)\n",
    "pprint.pprint(chunk_file_text.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = _vectors.create_vector_store(store_name=\"SolarSystem\", store_type=\"opensearchserverless\", description=\"Collection for storing vectorized documents\", tags=[{\"key\": \"project\", \"value\": \"GenerativeAI\"}])\n",
    "pprint.pprint(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Vector Store Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _vectors.get_vector_store_status(store_id=vector_store['store_id'])\n",
    "pprint.pprint(response)\n",
    "\n",
    "while response['status'] != 'ACTIVE':\n",
    "    response = _vectors.get_vector_store_status(store_id=vector_store['store_id'])\n",
    "    pprint.pprint(response)\n",
    "    time.sleep(5)\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"my_index\"\n",
    "vector_index = _vectors.create_vector_index(store_id=vector_store['store_id'], index_name=index_name)\n",
    "pprint.pprint(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if index is ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_status = _vectors.get_vector_index_status(index_id=vector_index['index_id'])\n",
    "while index_status['status'] != 'ACTIVE':\n",
    "    index_status = _vectors.get_vector_index_status(index_id=vector_index['index_id'])\n",
    "    pprint.pprint(index_status)\n",
    "    time.sleep(5)\n",
    "\n",
    "pprint.pprint(index_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_job = _vectors.vectorize(chunking_job_id=chunk_job['chunking_job_id'], index_id=vector_index['index_id'])\n",
    "pprint.pprint(vectorize_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Vectorization Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_job = _vectors.get_vectorize_job_status(vectorize_job_id=vectorize_job['vectorize_job_id'])\n",
    "while vectorize_job['status'] != 'COMPLETED' and vectorize_job['status'] != 'FAILED' and vectorize_job['status'] != 'COMPLETED_WITH_ERRORS':\n",
    "    vectorize_job = _vectors.get_vectorize_job_status(vectorize_job['vectorize_job_id'])\n",
    "    pprint.pprint(vectorize_job)\n",
    "    time.sleep(5)\n",
    "\n",
    "pprint.pprint(vectorize_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _vectors.semantic_search(query=\"<REPLACE_WITH_YOUR_QUERY>\", index_id=vector_index['index_id'])\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"<REPLACE_WITH_YOUR_QUERY>\"\n",
    "prompt = \"\"\"\n",
    "           You are a helpful assistant. Given a context, answer the following question.\n",
    "           Context: {context}\n",
    "           Question: {question}\n",
    "           Answer:\n",
    "           \"\"\"\n",
    "# Vector search\n",
    "response = _vectors.semantic_search(query=question, index_id=vector_index['index_id'])\n",
    "print(response)\n",
    "context_text = \"\"\n",
    "for hit in response:\n",
    "    context_text += hit['text'] + ' '\n",
    "    context_text += \" \"\n",
    "final_prompt = prompt.format(context=context_text, question=question)\n",
    "\n",
    "response = _model.invoke_model(model_name=\"ANTHROPIC_CLAUDE_V2\", prompt=final_prompt, max_tokens=100, temperature=0.7, top_p=0.9, top_k=50, stop_sequences=[\"\\\\n\"])\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
